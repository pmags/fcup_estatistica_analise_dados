---
editor_options:
  chunk_output_type: console
---


```{r  echo=FALSE, message=FALSE, warning=FALSE, cache=FALSE}

#source("scripts/eda.R", local = TRUE, encoding = "UTF-8")
source("./scripts/univariable_eda.R", local = TRUE, encoding = "UTF-8")

source("./scripts/pca.R", local = TRUE, encoding = "UTF-8")

knitr::opts_chunk$set(fig.align="center", echo=FALSE, fig.show = "hold")

```


# Análise Componentes Principais

```{r}

do.call(rbind.data.frame, chi_test_factors)

```
Analisando os testes qui-quadrados de todas as combinações das variáveis rejeitamos todas as hipóteses nulas correspondentes aos pares de variáveis analisadas nos testes qui-quadrados, pois **p-value < 0.05**. A hipótese Ho rejeitada afirma que o par te variáveis analisada no teste são idependentes.


```{r}


pca <- PCA(data %>% select_if(is.numeric), ncp = 7)


```

As correlações geralmente são representadas em gráficos de círculo de correlações. No gráfico acima, temos o círculo de correlação entre a componente 1 e 2. O interessante nesse gráfico é analisar as correlações que se opõe via sinal. Dessa forma, podemos ver pela primeira componente que todos os ppm (ouro, platina, paládio, ródio, irídio e ruténio) e o óxido de alumínio são opostos ao fósforo e óxido de magnésio, ou seja, notamos que amostras tem maior concentração distintas desses elementos, os primeiros mencionados em maior concentração e os últimos em menor concentração.

Já a componente 2 nos mostra uma oposição entre profundidade, óxido de cromo e óxido de ferro em oposição a óxido de cálcio e óxido de silício seguindo a mesma ideia da explicação anterior.


## Eigenvectors
```{r}

pca$svd$V # Eigenvectors

```
Podemos ver a tabela acima os 7 primeiros autovetores correspondentes aos autovalores em ordem decrescente. Sabemos que os autovetores estão normalizados e isso significa que a soma dos quadrados de cada componente do vetor é igual a 1. E, sabemos que eles são ortogonais, pois o produto de quaisquer dois autovetores, isto é a soma das componentes dos vetores multiplicadas termo a termo é sempre igual a zero. E isso significa que as componentes não estão correlacionadas entre si.

Com os autovetores nós conseguimos escrever as equações das componentes principais como combinação linear dos autovetores e as variáveis originais normalizadas.


## Eigenvalues
```{r}
pca$eig # Eigenvalues

```
Analisando as medidas de correlação, pois estamos fazendo o PCA normatizado o que se justifica dado que algumas variáveis possuem unidades muito diferentes e dispersão muito diferente. Nós obtemos a lista dos 16 autovalores apresentados aqui de forma decrescente.

Ao analisarmos os autovalores, temos que se utilizarmos o critério de Kaiser, onde diz que devemos manter as componentes com autovalores acima de 1, **reteríamos apenas as 6 primeiras componentes principais**, pois dessa forma reteríamos as componentes mais informativas do que as variáveis originais, dado que estas possuem variância maior do que as originais. 

Mas se considerarmos o critério de Pearson, onde diz devemos manter as componentes de forma que elas expliquem pelo menos 80% da dispersão total, aí teríamos que **manter as 7 primeiras componentes principais** para atingir esse objetivo.



```{r}

barplot(pca$eig[,1],main="Eigenvalues",names.arg=1:nrow
(pca$eig))

```
Aqui temos um histograma onde podemos visualizar a amplitude de cada um dos auto valores e podemos ver que os 3 primeiros realmente se destacam dos demais já os seguintes vão diminuindo suas amplitudes gradativamente sem nenhuma alta discrepância de maneira a formar degraus como os 3 primeiros.

## Visualização das dimensões x percentagem de explicação da variância
```{r}
fviz_eig(pca, addlabels = TRUE, ylim = c(0, 50))
```


## Correlação entre as componentes principais e as variáveis originais
```{r}

round(pca$var$cor, 3)

```
Analisando os resultados de correlação entre as componentes principais e as variáveis originais da nossa base de dados, podemos concluir que:
  
  1. A componente principal 1 (*Dim 1*) tem correlação forte negativa com as variáveis **DepthFrom, DepthTo, Cr2O3_%** e correlação forte positiva com as variáveis **CaO_%, SiO2_%, Pt_ICP_ppm, Pd_ICP_ppm, Rh_ICP_ppm, Ir_ICP_ppm**. Esta componente representa amostras com maior concentração de óxido de cromo, óxido de silício, ppm de platina, ppm de paládio, ppm de ródio e ppm de irídio, mas também com uma menor concentração de óxido de cromo e ainda representam amostras que foram coletadas em profundidades baixas, mais próximas da superfície.
  
  2. A componente principal 2 (*Dim 2*) possui correlação forte negativa com as seguintes variável **SiO2_%** e correlação forte positiva com as variáveis **Cr2O3_%, FeO_%, Ir_ICP_ppm, Ru_ICP_ppm**. O que representa somente elementos químicos presentes na análise de uma amostra de cromitita, são melhores representadas pela Dim 2 quando olhamos para as correlações. Sendo assim essa componente representa amostras com maior concentração de óxido de cromo, óxido de ferro, ppm de ruténio e ppm de irídio, mas também com menor concentração de óxido de silício.
  
  3. Para a componente principal 3 (*Dim 3*) temos uma apenas correlação forte positiva com as variáveis **DepthFrom, DepthTo, MgO_%**. Sendo assim essa componente identifica amostras com maiores concentrações de óxido de magnésio e amostras retiradas de locais mais profundos, mais distantes da superfície.
  
  4. Para a componente principal 4 (*Dim 4*) vemos uma maior correlação positiva da variável P_% e uma correlação forte negativa da variável **Al2O3_%** então essa componente descreve amostras com maiores concentrações de fósforo e menores concentrações de óxido de alumínio.
  
  5. Para a componente principal 5 (*Dim 5*) podemos ver que temos uma maior correlação positiva com a variável **Au_ICP_ppm**, desta forma essa componente consegue identificar amostras com uma maior concentração de ppm de ouro.
  
  6. Para a componente principal 6 (*Dim 6*) temos uma correlação forte positiva com a variável **P_%**, sendo assim essa componente consegue representar amostras com maior concentração de fósforo.
  
  7. E por fim a componente principal 7 (*Dim 7*) não possui correlação forte nem positiva nem negativa com nenhuma das variáveis originais.

## Contribuição presente nas componentes principais
```{r}

round(pca$var$contrib, 3)

```
Aqui podemos ver que na componente principal 1 (*Dim 1*) as variáveis que representam a profundidade, óxido de cromo, ppm de platina, ppm de ródio e ppm de irídio correspondem as variáveis que tem contribuições mais importantes para a componente e ainda são as mesmas variáveis que possuem maiores correlação. Analogamente essa análise pode ser feita para as demais componentes, seguindo o mesmo raciocínio.

## Gráfico PCA dos indivíduos 
```{r}
plot(pca, axes = c(1,2))
```
No gráfico de indivíduos acima conseguimos observar que os indivíduos **1136 e 651** tem alta concentração dos ppm (ouro, platina, paládio, ródio, irídio e ruténio) e de óxido de alumínio isso porque a componente 1 nos mostra isso. E, os indivíduos **534, 745 e 160** tem menores concentrações de óxido de cálcio e óxido de silício. Os outros indivíduos como estão bastante concentrados nos quadrantes de 1 a 4 não conseguimos os identificar para uma análise apropriada.

```{r}
plot(pca, axes = c(3,4))
```
Aqui temos que o indivíduo **881** tem uma alta concetração de óxido de ferro e fósforo. Enquanto os indíviduos **657 e 1143** tem uma alta concentração de ppm de ouro, ppm de paládio e óxido de manganês.

```{r}
plot(pca, axes = c(5,6))
```
Já nesté gráfico dos indivíduos representados pelas componentes 5 e 6, podemos observar que o indivíduo **881** tem uma alta concentração de fósforo indicada pela componente 6, os indivíduos **275, 105 e 647** tem uma alta concentração de ppm de outro indicada pela componente 5.


Pegando as componentes mais importantes para utilizar na sequência nos métodos de clusterização e classificação
```{r}
var = as_tibble(factoextra::get_pca_ind(pca)$coord)
```

K-means plot método do cutuvelo
```{r}
d = daisy(var)
resultados = rep(0, 14)
for (i in c(2,3,4,5,6,7,8,9,10,11,12,13,14,15))
{
  fit           = kmeans(var, i)
  y_cluster     = fit$cluster
  sk            = silhouette(y_cluster, d)
  resultados[i] = fit$tot.withinss
}
plot(2:15,resultados[2:15],type="o",col="purple",pch=0,xlab="Número de clusters",ylab="tot.tot.withinss")
```

Continuamos testando, com a função kmeansruns com os critérios asw (silhueta média) e ch (Calinski-Harabasz), que estima a razão entre a distância entre clusters e a distância intra-cluster, quanto maior a razão, melhor o modelo.
```{r}
fit_ch  = kmeansruns(var, krange = 2:15, criterion = "ch") 
fit_asw = kmeansruns(var, krange = 2:15, criterion = "asw") 
```

```{r}
fit_ch$bestk
fit_asw$bestk
```


Plotando os gráficos para os métodos aplicados
```{r}
plot(2:16, fit_asw$crit, type="o",col="purple",pch=0,xlab="Número de clústers",ylab="Criterio silueta media")
```

```{r}
plot(2:16,fit_ch$crit,type="o",col="purple",pch=0,xlab="Número de clústers",ylab="Criterio Calinski-Harabasz")
```
Vemos que os valores são 3 para ambos os métodos Calinski-Harabasz e para a silhueta média, mostrados nos gráficos.


Vamos primeiro realizar o aprendizado para k=3
```{r}
kmeansclusters = kmeans(var, 3)

# Kernel length and width
plot(var[c(1,2)], col=kmeansclusters$cluster)
```

```{r}
plot(var[c(2,4)], col=kmeansclusters$cluster)
```


```{r}
table(kmeansclusters$cluster, data$MaxDepth)
```

Para avaliar ainda mais o modelo obteremos a qualidade com o parâmetro Silhouette que determina a coesão dos clusters, medindo a similaridade de cada dado com os de seu cluster em relação aos dados de outros clusters. Observamos que não há valores negativos, indicando que os clusters são bem coesos.

Vamos agora inspecionar os clusters.

Vamos também inspecionar os gráficos de silhueta, vemos alguns valores abaixo de zero, o que indica boa qualidade no modelo. O valor médio da silhueta também é um bom indicador, vamos compará-lo com os demais modelos.
```{r}
library(factoextra)
sil = silhouette(kmeansclusters$cluster,daisy(var))
fviz_silhouette(sil,label=FALSE,print.summary=FALSE)
```

Por fim obteremos o índice dunn que também realiza uma avaliação obtendo uma razão entre a distância mínima dos dados em diferentes clusters e a distância máxima intra-cluster. Quanto maior o dunn, melhor a coesão dos clusters.
```{r}
clust_stats = cluster.stats(d, kmeansclusters$cluster)
clust_stats$dunn
```

```{r}
kmeansclusters = kmeans(var, 8)
```

```{r}
library(factoextra)
sil = silhouette(kmeansclusters$cluster,daisy(var))
fviz_silhouette(sil, label=FALSE,print.summary=FALSE)
```


```{r}
clust_stats = cluster.stats(d, kmeansclusters$cluster)
clust_stats$dunn
```


```{r}
num_clusters = fviz_nbclust(var, kmeans, k.max = 15, 
          method = "wss", linecolor ="steelblue")
num_clusters
```


```{r}
fit2 = hclust(d, method="ward.D2")
```


```{r}
fviz_dend(fit2, k = 3, # Cut in four groups
          cex = 0.5, # label size
          k_colors = c("#2E9FDF", "#00AFBB", "#E7B800"),
          color_labels_by_k = TRUE, # color labels by groups
          rect = TRUE # Add rectangle around groups
          )
```


```{r}
fit1 = Mclust(var)
summary(fit1)
```

```{r}
# BIC values used to choose the number of clusters.
fviz_mclust(fit1, "BIC", palette = "jco")
```

```{r}
# Classification 
fviz_mclust(fit1, "classification", geom = "point", 
            pointsize = 1.5, palette = "jco")

```

```{r}
sil = silhouette(fit1$classification,d)
fviz_silhouette(sil,label=FALSE,print.summary=FALSE)
```


```{r}
table(fit3$classification,data$MaxDepth)
```

```{r}
clust_stats = cluster.stats(d,fit3$classification)
clust_stats$dunn
```


Pegando as componentes mais importantes para utilizar na sequência nos métodos de clusterização e classificação


```{r}
var = data[c("Cr2O3_%", "FeO_%", "SiO2_%", "MgO_%", "Al2O3_%", "CaO_%", "P_%", 
             "Au_ICP_ppm", "Pt_ICP_ppm", "Pd_ICP_ppm", "Rh_ICP_ppm", 
             "Ir_ICP_ppm", "Ru_ICP_ppm")]
```

```{r}
var[is.na(var)] = 0
any(is.na(var))
```


K-means plot método do cutuvelo
```{r}
d = daisy(var)
resultados = rep(0, 14)
for (i in c(2,3,4,5,6,7,8,9,10,11,12,13,14,15))
{
  fit           = kmeans(var, i)
  y_cluster     = fit$cluster
  sk            = silhouette(y_cluster, d)
  resultados[i] = fit$tot.withinss
}
plot(2:15,resultados[2:15],type="o",col="purple",pch=0,xlab="Número de clusters",ylab="tot.tot.withinss")
```

Continuamos testando, com a função kmeansruns com os critérios asw (silhueta média) e ch (Calinski-Harabasz), que estima a razão entre a distância entre clusters e a distância intra-cluster, quanto maior a razão, melhor o modelo.
```{r}
fit_ch  = kmeansruns(var, krange = 2:15, criterion = "ch") 
fit_asw = kmeansruns(var, krange = 2:15, criterion = "asw") 
```

```{r}
fit_ch$bestk
fit_asw$bestk
```


Plotando os gráficos para os métodos aplicados
```{r}
plot(2:16, fit_asw$crit, type="o",col="purple",pch=0,xlab="Número de clústers",ylab="Criterio silueta media")
```

```{r}
plot(2:16,fit_ch$crit,type="o",col="purple",pch=0,xlab="Número de clústers",ylab="Criterio Calinski-Harabasz")
```
Vemos que os valores são 3 para ambos os métodos Calinski-Harabasz e para a silhueta média, mostrados nos gráficos.


Vamos primeiro realizar o aprendizado para k=3
```{r}
kmeansclusters = kmeans(var, 3)

# Kernel length and width
plot(var[c(1,2)], col=kmeansclusters$cluster)
```

```{r}
plot(var[c(2,4)], col=kmeansclusters$cluster)
```


```{r}
table(kmeansclusters$cluster, data$MaxDepth)
```

Para avaliar ainda mais o modelo obteremos a qualidade com o parâmetro Silhouette que determina a coesão dos clusters, medindo a similaridade de cada dado com os de seu cluster em relação aos dados de outros clusters. Observamos que não há valores negativos, indicando que os clusters são bem coesos.

Vamos agora inspecionar os clusters.

Vamos também inspecionar os gráficos de silhueta, vemos alguns valores abaixo de zero, o que indica boa qualidade no modelo. O valor médio da silhueta também é um bom indicador, vamos compará-lo com os demais modelos.
```{r}
library(factoextra)
sil = silhouette(kmeansclusters$cluster,daisy(var))
fviz_silhouette(sil,label=FALSE,print.summary=FALSE)
```

Por fim obteremos o índice dunn que também realiza uma avaliação obtendo uma razão entre a distância mínima dos dados em diferentes clusters e a distância máxima intra-cluster. Quanto maior o dunn, melhor a coesão dos clusters.
```{r}
clust_stats = cluster.stats(d, kmeansclusters$cluster)
clust_stats$dunn
```


```{r}
fit1 = Mclust(var)
summary(fit1)
```

```{r}
# BIC values used to choose the number of clusters.
fviz_mclust(fit1, "BIC", palette = "jco")
```

```{r}
# Classification 
fviz_mclust(fit1, "classification", geom = "point", 
            pointsize = 1.5, palette = "jco")

```

```{r}
sil = silhouette(fit1$classification,d)
fviz_silhouette(sil,label=FALSE,print.summary=FALSE)
```


```{r}
table(fit1$classification,data$MaxDepth)
```

```{r}
clust_stats = cluster.stats(d,fit1$classification)
clust_stats$dunn
```


```{r}
# K-means clustering
km.res = eclust(var, "kmeans", k = 3, nstart = 25, graph = FALSE)
# Visualize k-means clusters
fviz_cluster(km.res, geom = "point", ellipse.type = "norm",
             palette = "jco", ggtheme = theme_minimal())
```


```{r}
# Hierarchical clustering
hc.res = eclust(var, "hclust", k = 3, hc_metric = "euclidean", 
                 hc_method = "ward.D2", graph = FALSE)

# Visualize dendrograms
fviz_dend(hc.res, show_labels = FALSE,
         palette = "jco", as.ggplot = TRUE)
```


```{r}
# Silhouette information
silinfo = km.res$silinfo
names(silinfo)
# Silhouette widths of each observation
head(silinfo$widths[, 1:3], 10)
# Average silhouette width of each cluster
silinfo$clus.avg.widths
# The total average (mean of all individual silhouette widths)
silinfo$avg.width
# The size of each clusters
km.res$size
# Silhouette width of observation
sil = km.res$silinfo$widths[, 1:3]
# Objects with negative silhouette
neg_sil_index = which(sil[, 'sil_width'] < 0)
sil[neg_sil_index, , drop = FALSE]
```


```{r}
cluster.stats(d = NULL, clustering, al.clustering = NULL)
# Statistics for k-means clustering
km_stats = cluster.stats(dist(var),  km.res$cluster)
# Dun index
km_stats$dunn

km_stats
```


```{r}
table(data$MaxDepth, km.res$cluster)
```


```{r}
# Compute cluster stats
maxdepth = as.numeric(data$MaxDepth)
clust_stats = cluster.stats(d = dist(var), 
                             maxdepth, km.res$cluster)
# Corrected Rand index
clust_stats$corrected.rand
```


```{r}
clust_stats$vi
```


```{r}
var = scale(var)
# Agreement between species and pam clusters
pam.res = eclust(var, "pam", k = 3, graph = FALSE)
table(data$MaxDepth, pam.res$cluster)
cluster.stats(d = dist(var), 
              maxdepth, pam.res$cluster)$vi
# Agreement between species and HC clusters
res.hc = eclust(var, "hclust", k = 3, graph = FALSE)
table(data$MaxDepth, res.hc$cluster)
cluster.stats(d = dist(var), 
              maxdepth, res.hc$cluster)$vi
```


Referências
https://www.datanovia.com/en/lessons/cluster-validation-statistics-must-know-methods/


\newpage

